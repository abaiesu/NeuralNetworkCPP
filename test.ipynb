{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:22:00.285511: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 12:22:00.545318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742124120.627456   58871 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742124120.644305   58871 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742124120.763213   58871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742124120.763246   58871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742124120.763247   58871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742124120.763248   58871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-16 12:22:00.783884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abaiesu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abaiesu/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-03-16 12:22:04.989787: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7684  \n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8987 \n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3680 \n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6386 \n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3020 \n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1441 \n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0697 \n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0360 \n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0201 \n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119 \n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0078 \n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0056 \n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045 \n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 \n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0032 \n",
      "Epoch 16/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 \n",
      "Epoch 17/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0023 \n",
      "Epoch 18/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022 \n",
      "Epoch 19/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019 \n",
      "Epoch 20/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018 \n",
      "Epoch 21/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016 \n",
      "Epoch 22/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015 \n",
      "Epoch 23/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0013\n",
      "Epoch 24/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0011\n",
      "Epoch 25/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0010     \n",
      "Epoch 26/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8096e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7229e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8753e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7241e-04 \n",
      "Epoch 30/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1994e-04 \n",
      "Epoch 31/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6252e-04 \n",
      "Epoch 32/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0088e-04 \n",
      "Epoch 33/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2865e-04 \n",
      "Epoch 34/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7905e-04 \n",
      "Epoch 35/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4301e-04 \n",
      "Epoch 36/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1957e-04 \n",
      "Epoch 37/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8181e-04 \n",
      "Epoch 38/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6866e-04 \n",
      "Epoch 39/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4845e-04 \n",
      "Epoch 40/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1047e-04 \n",
      "Epoch 41/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9078e-04 \n",
      "Epoch 42/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7181e-04 \n",
      "Epoch 43/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5885e-04 \n",
      "Epoch 44/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4085e-04 \n",
      "Epoch 45/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2403e-04 \n",
      "Epoch 46/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1539e-04 \n",
      "Epoch 47/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0649e-04 \n",
      "Epoch 48/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0274e-05 \n",
      "Epoch 49/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0234e-05 \n",
      "Epoch 50/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5350e-05 \n",
      "Learned coefficients: C11 = 2.0205, C12 = 2.9801\n",
      "True coefficients: a = 2.0, b = 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the true coefficients a and b\n",
    "a = 2.0\n",
    "b = 3.0\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.random.rand(n_samples)\n",
    "z = a * x + b * y  # True z values\n",
    "\n",
    "# Combine x and y into input data\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, use_bias=False))  # Single layer with 2 inputs and 1 output (no bias)\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss='mean_squared_error')  # Stochastic Gradient Descent\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, z, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Get the learned weights (C11 and C12)\n",
    "C11, C12 = model.get_weights()[0].flatten()\n",
    "print(f\"Learned coefficients: C11 = {C11:.4f}, C12 = {C12:.4f}\")\n",
    "print(f\"True coefficients: a = {a}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1465, C11: 1.8006, C12: 2.1370\n",
      "Epoch [20/50], Loss: 0.0142, C11: 2.1693, C12: 2.6677\n",
      "Epoch [30/50], Loss: 0.0028, C11: 2.1822, C12: 2.7968\n",
      "Epoch [40/50], Loss: 0.0012, C11: 2.1489, C12: 2.8517\n",
      "Epoch [50/50], Loss: 0.0010, C11: 2.1163, C12: 2.8868\n",
      "\n",
      "Training complete!\n",
      "Learned coefficients: C11 = 2.1163, C12 = 2.8868\n",
      "True coefficients: a = 2.0, b = 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# True coefficients\n",
    "a = 2.0\n",
    "b = 3.0\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.random.rand(n_samples)\n",
    "z = a * x + b * y  # True z values\n",
    "\n",
    "# Combine x and y into input data\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "# Initialize weights (C11 and C12)\n",
    "C11 = np.random.randn()\n",
    "C12 = np.random.randn()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuffled = X[indices]\n",
    "    z_shuffled = z[indices]\n",
    "\n",
    "    # Mini-batch SGD\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        # Get a mini-batch\n",
    "        X_batch = X_shuffled[i:i + batch_size]\n",
    "        z_batch = z_shuffled[i:i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        z_pred = C11 * X_batch[:, 0] + C12 * X_batch[:, 1]\n",
    "\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = np.mean(0.5 * (z_pred - z_batch) ** 2)\n",
    "\n",
    "        # Backpropagation\n",
    "        d_loss_dz_pred = (z_pred - z_batch) / batch_size  # Gradient of loss w.r.t. z_pred\n",
    "        d_loss_dC11 = np.sum(d_loss_dz_pred * X_batch[:, 0])  # Gradient of loss w.r.t. C11\n",
    "        d_loss_dC12 = np.sum(d_loss_dz_pred * X_batch[:, 1])  # Gradient of loss w.r.t. C12\n",
    "\n",
    "        # Update weights using SGD\n",
    "        C11 -= learning_rate * d_loss_dC11\n",
    "        C12 -= learning_rate * d_loss_dC12\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, C11: {C11:.4f}, C12: {C12:.4f}\")\n",
    "\n",
    "# Final results\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Learned coefficients: C11 = {C11:.4f}, C12 = {C12:.4f}\")\n",
    "print(f\"True coefficients: a = {a}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 0.362554\n",
      "Epoch 2/50 - Loss: 0.001398\n",
      "Epoch 3/50 - Loss: 0.000259\n",
      "Epoch 4/50 - Loss: 0.000048\n",
      "Epoch 5/50 - Loss: 0.000009\n",
      "Epoch 6/50 - Loss: 0.000002\n",
      "Epoch 7/50 - Loss: 0.000000\n",
      "Epoch 8/50 - Loss: 0.000000\n",
      "Epoch 9/50 - Loss: 0.000000\n",
      "Epoch 10/50 - Loss: 0.000000\n",
      "Epoch 11/50 - Loss: 0.000000\n",
      "Epoch 12/50 - Loss: 0.000000\n",
      "Epoch 13/50 - Loss: 0.000000\n",
      "Epoch 14/50 - Loss: 0.000000\n",
      "Epoch 15/50 - Loss: 0.000000\n",
      "Epoch 16/50 - Loss: 0.000000\n",
      "Epoch 17/50 - Loss: 0.000000\n",
      "Epoch 18/50 - Loss: 0.000000\n",
      "Epoch 19/50 - Loss: 0.000000\n",
      "Epoch 20/50 - Loss: 0.000000\n",
      "Epoch 21/50 - Loss: 0.000000\n",
      "Epoch 22/50 - Loss: 0.000000\n",
      "Epoch 23/50 - Loss: 0.000000\n",
      "Epoch 24/50 - Loss: 0.000000\n",
      "Epoch 25/50 - Loss: 0.000000\n",
      "Epoch 26/50 - Loss: 0.000000\n",
      "Epoch 27/50 - Loss: 0.000000\n",
      "Epoch 28/50 - Loss: 0.000000\n",
      "Epoch 29/50 - Loss: 0.000000\n",
      "Epoch 30/50 - Loss: 0.000000\n",
      "Epoch 31/50 - Loss: 0.000000\n",
      "Epoch 32/50 - Loss: 0.000000\n",
      "Epoch 33/50 - Loss: 0.000000\n",
      "Epoch 34/50 - Loss: 0.000000\n",
      "Epoch 35/50 - Loss: 0.000000\n",
      "Epoch 36/50 - Loss: 0.000000\n",
      "Epoch 37/50 - Loss: 0.000000\n",
      "Epoch 38/50 - Loss: 0.000000\n",
      "Epoch 39/50 - Loss: 0.000000\n",
      "Epoch 40/50 - Loss: 0.000000\n",
      "Epoch 41/50 - Loss: 0.000000\n",
      "Epoch 42/50 - Loss: 0.000000\n",
      "Epoch 43/50 - Loss: 0.000000\n",
      "Epoch 44/50 - Loss: 0.000000\n",
      "Epoch 45/50 - Loss: 0.000000\n",
      "Epoch 46/50 - Loss: 0.000000\n",
      "Epoch 47/50 - Loss: 0.000000\n",
      "Epoch 48/50 - Loss: 0.000000\n",
      "Epoch 49/50 - Loss: 0.000000\n",
      "Epoch 50/50 - Loss: 0.000000\n",
      "Learned coefficients: C11 = 2.0000, C12 = 3.0000\n",
      "True coefficients: a = 2.0, b = 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define the Dense layer\n",
    "# -----------------------------\n",
    "class Dense:\n",
    "    \"\"\"\n",
    "    A simple Dense layer with no bias for demonstration.\n",
    "    This layer has 'input_dim' inputs and 'output_dim' outputs.\n",
    "    Weights shape: (input_dim, output_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights using a small random normal distribution\n",
    "        # with no bias (similar to 'use_bias=False' in Keras)\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: just a dot product between inputs and weights\n",
    "        x shape: (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        self.x = x  # store for backprop\n",
    "        return x.dot(self.weights)  # (batch_size, output_dim)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass:\n",
    "         - grad_output: gradient from the next layer (or loss), shape: (batch_size, output_dim)\n",
    "         - we need to compute:\n",
    "              grad_input = grad_output dot W^T\n",
    "              grad_weights = X^T dot grad_output\n",
    "        \"\"\"\n",
    "        # Gradient w.r.t. input\n",
    "        grad_input = grad_output.dot(self.weights.T)  # (batch_size, input_dim)\n",
    "        \n",
    "        # Gradient w.r.t. weights\n",
    "        self.grad_weights = self.x.T.dot(grad_output)  # (input_dim, output_dim)\n",
    "        return grad_input\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights with gradient descent:\n",
    "        weights = weights - learning_rate * grad_weights\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.grad_weights\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Define the Mean Squared Error loss\n",
    "# -----------------------------\n",
    "def mse_loss(predictions, targets):\n",
    "    \"\"\"\n",
    "    Mean squared error loss.\n",
    "    predictions: (batch_size, 1)\n",
    "    targets: (batch_size,)\n",
    "    Returns scalar loss and gradient w.r.t. predictions\n",
    "    \"\"\"\n",
    "    # Ensure targets has shape (batch_size, 1) for broadcast\n",
    "    targets = targets.reshape(-1, 1)\n",
    "    errors = predictions - targets\n",
    "    loss = 0.5 * (np.mean(errors**2))\n",
    "    \n",
    "    # Gradient of MSE w.r.t. predictions = 2*(predictions - targets)/N\n",
    "    grad = errors\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Define the Network\n",
    "# -----------------------------\n",
    "class SimpleNetwork:\n",
    "    def __init__(self):\n",
    "        # For our problem: input_dim=2, output_dim=1, single Dense layer\n",
    "        self.layer = Dense(input_dim=2, output_dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the single layer\n",
    "        \"\"\"\n",
    "        return self.layer.forward(x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass through the single layer\n",
    "        \"\"\"\n",
    "        return self.layer.backward(grad_output)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights in the layer\n",
    "        \"\"\"\n",
    "        self.layer.update(learning_rate)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Define a function to train with SGD\n",
    "# -----------------------------\n",
    "def train_network(network, X, y, epochs=50, batch_size=32, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train the 'network' with mini-batch gradient descent on dataset (X, y).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        # Mini-batch training\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            x_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            preds = network.forward(x_batch)\n",
    "            \n",
    "            # Compute loss and gradient\n",
    "            loss, grad = mse_loss(preds, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            network.backward(grad)\n",
    "            \n",
    "            # Update weights\n",
    "            network.update(learning_rate)\n",
    "        \n",
    "        # Print average loss for the epoch\n",
    "        avg_loss = epoch_loss / (n_samples // batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Generate synthetic data\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# True coefficients\n",
    "a = 2.0\n",
    "b = 3.0\n",
    "\n",
    "# Random inputs\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.random.rand(n_samples)\n",
    "\n",
    "# True outputs: z = a*x + b*y\n",
    "z = a * x + b * y\n",
    "\n",
    "# Prepare data for training\n",
    "X = np.column_stack((x, y))  # shape (1000, 2)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Create the network and train\n",
    "# -----------------------------\n",
    "network = SimpleNetwork()\n",
    "train_network(network, X, z, epochs=50, batch_size=32, learning_rate=0.01)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Check learned weights\n",
    "# -----------------------------\n",
    "learned_weights = network.layer.weights\n",
    "C11, C12 = learned_weights[:,0]\n",
    "\n",
    "print(f\"Learned coefficients: C11 = {C11:.4f}, C12 = {C12:.4f}\")\n",
    "print(f\"True coefficients: a = {a}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 2.738100\n",
      "Epoch 2/30 - Loss: 1.984933\n",
      "Epoch 3/30 - Loss: 1.448381\n",
      "Epoch 4/30 - Loss: 1.049908\n",
      "Epoch 5/30 - Loss: 0.766015\n",
      "Epoch 6/30 - Loss: 0.563897\n",
      "Epoch 7/30 - Loss: 0.411514\n",
      "Epoch 8/30 - Loss: 0.302003\n",
      "Epoch 9/30 - Loss: 0.223035\n",
      "Epoch 10/30 - Loss: 0.161690\n",
      "Epoch 11/30 - Loss: 0.121821\n",
      "Epoch 12/30 - Loss: 0.089695\n",
      "Epoch 13/30 - Loss: 0.067503\n",
      "Epoch 14/30 - Loss: 0.050323\n",
      "Epoch 15/30 - Loss: 0.038127\n",
      "Epoch 16/30 - Loss: 0.029545\n",
      "Epoch 17/30 - Loss: 0.022624\n",
      "Epoch 18/30 - Loss: 0.017630\n",
      "Epoch 19/30 - Loss: 0.013911\n",
      "Epoch 20/30 - Loss: 0.011159\n",
      "Epoch 21/30 - Loss: 0.008932\n",
      "Epoch 22/30 - Loss: 0.007210\n",
      "Epoch 23/30 - Loss: 0.005969\n",
      "Epoch 24/30 - Loss: 0.004874\n",
      "Epoch 25/30 - Loss: 0.004153\n",
      "Epoch 26/30 - Loss: 0.003573\n",
      "Epoch 27/30 - Loss: 0.003024\n",
      "Epoch 28/30 - Loss: 0.002572\n",
      "Epoch 29/30 - Loss: 0.002290\n",
      "Epoch 30/30 - Loss: 0.001955\n",
      "\n",
      "Learned coefficients: C11 = 2.0791, C12 = 2.8805\n",
      "True coefficients: a = 2.0, b = 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Activation Layer\n",
    "#    (Here we implement a generic interface plus\n",
    "#     an IdentityActivation as an example.)\n",
    "# ----------------------------------------------------\n",
    "class ActivationLayer:\n",
    "    \"\"\"\n",
    "    Activation layer: X^{(ell)}_i = f^{(ell)}(X^{(ell-1)}_i).\n",
    "    This layer has no trainable parameters.\n",
    "    \"\"\"\n",
    "    def forward(self, X_prev):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          X_prev is of shape (batch_size, n_{ell-1})\n",
    "          Returns X^(ell) of shape (batch_size, n_{ell-1})\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, dX):\n",
    "        \"\"\"\n",
    "        Backward pass:\n",
    "          dX is the gradient of the loss w.r.t. this layer’s outputs, shape: (batch_size, n_{ell-1})\n",
    "          Returns gradient w.r.t. this layer’s inputs\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class IdentityActivation(ActivationLayer):\n",
    "    \"\"\"\n",
    "    Identity activation: f(x) = x\n",
    "    f'(x) = 1\n",
    "    \"\"\"\n",
    "    def forward(self, X_prev):\n",
    "        self.X_prev = X_prev  # store for backward\n",
    "        return X_prev\n",
    "\n",
    "    def backward(self, dX):\n",
    "        # derivative of identity is 1, so the gradient is passed through\n",
    "        return dX\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Connection Layer (Linear layer)\n",
    "#    X^{(ell)} = W^{(ell)} * X^{(ell-1)}\n",
    "# ----------------------------------------------------\n",
    "class ConnectionLayer:\n",
    "    \"\"\"\n",
    "    A connection (linear) layer without bias:\n",
    "      X^{(ell)} = W^{(ell)} * X^{(ell-1)}\n",
    "\n",
    "    By convention here, W^{(ell)} is shaped (n_{ell}, n_{ell-1}),\n",
    "    which matches the formula: X^{(ell)} = W^{(ell)} X^{(ell-1)}.\n",
    "    However, we often store data as row-vectors in code:\n",
    "      - X^{(ell-1)} is shape (batch_size, n_{ell-1})\n",
    "      - W^{(ell)} is shape (n_{ell-1}, n_{ell})\n",
    "      - So typically in code we compute: X^{(ell)} = X^{(ell-1)} dot W^{(ell)}\n",
    "    \n",
    "    To stay consistent with the problem statement, we can store W^(ell)\n",
    "    as (n_{ell}, n_{ell-1}) but then do a transpose in the forward pass.\n",
    "    \n",
    "    For simplicity, we will store W^(ell) as (n_{ell-1}, n_{ell}) in code.\n",
    "    The math is the same; just note it’s the transpose of the statement’s formula.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize W^(ell) with small random values\n",
    "        # shape (input_dim, output_dim)\n",
    "        self.W = 0.01 * np.random.randn(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, X_prev):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          X_prev: shape (batch_size, input_dim)\n",
    "          returns X_curr: shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.X_prev = X_prev  # store for backprop\n",
    "        # X^{(ell)} = X^{(ell-1)} dot W^(ell)\n",
    "        return X_prev.dot(self.W)\n",
    "\n",
    "    def backward(self, dX_curr):\n",
    "        \"\"\"\n",
    "        Backward pass:\n",
    "         - dX_curr: gradient of the loss w.r.t. outputs of this layer, shape: (batch_size, output_dim)\n",
    "         - We compute:\n",
    "             dW = X_prev^T dot dX_curr\n",
    "             dX_prev = dX_curr dot W^T\n",
    "        \"\"\"\n",
    "        # Gradient w.r.t. parameters W\n",
    "        self.dW = self.X_prev.T.dot(dX_curr)  # shape (input_dim, output_dim)\n",
    "\n",
    "        # Gradient w.r.t. inputs X_prev\n",
    "        dX_prev = dX_curr.dot(self.W.T)       # shape (batch_size, input_dim)\n",
    "        return dX_prev\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Gradient descent update:\n",
    "          W = W - learning_rate * dW\n",
    "        \"\"\"\n",
    "        self.W -= learning_rate * self.dW\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Loss Layer (Moindres Carrés / MSE)\n",
    "#    For each sample:\n",
    "#       J(W, x, y) = 1/2 * ||hat y_W - y||^2\n",
    "#    => ∂J/∂hat y_W = (hat y_W - y)\n",
    "# ----------------------------------------------------\n",
    "class MSELossLayer:\n",
    "    def forward(self, X_last, y_true):\n",
    "        \"\"\"\n",
    "        X_last: shape (batch_size, out_dim) = predictions = hat y_W\n",
    "        y_true: shape (batch_size,) or (batch_size, out_dim)\n",
    "        \n",
    "        Returns the scalar cost (average over the batch).\n",
    "        \"\"\"\n",
    "        # Make sure y_true has shape (batch_size, 1) if out_dim=1\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "        self.X_last = X_last  # predictions\n",
    "        self.y_true = y_true\n",
    "\n",
    "        # 1/2 * mean( (hat y - y)^2 )\n",
    "        errors = X_last - y_true\n",
    "        cost = 0.5 * np.mean(errors**2)\n",
    "        return cost\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Return gradient of the cost w.r.t X_last.\n",
    "        ∂J/∂hat y_W = (hat y_W - y).\n",
    "        We also consider the 1/m factor if we used the mean.\n",
    "        \"\"\"\n",
    "        batch_size = self.X_last.shape[0]\n",
    "        dX_last = (self.X_last - self.y_true) / batch_size  # shape (batch_size, out_dim)\n",
    "        return dX_last\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Define a simple Network class\n",
    "# ----------------------------------------------------\n",
    "class SimpleNetwork:\n",
    "    def __init__(self, layers, loss_layer):\n",
    "        \"\"\"\n",
    "        layers: list of layers (ConnectionLayer or ActivationLayer)\n",
    "        loss_layer: an instance of a loss layer (e.g. MSELossLayer)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_layer = loss_layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers (except the loss).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dX):\n",
    "        \"\"\"\n",
    "        Backward pass through all layers in reverse order.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            dX = layer.backward(dX)\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        \"\"\"\n",
    "        Update all connection layers with gradient descent.\n",
    "        (Activation layers have no trainable parameters.)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ConnectionLayer):\n",
    "                layer.update_params(lr)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Training function with mini-batch SGD\n",
    "# ----------------------------------------------------\n",
    "def train_network(network, X, y, epochs=50, batch_size=32, lr=0.01):\n",
    "    \"\"\"\n",
    "    network: SimpleNetwork\n",
    "    X, y: training data\n",
    "    epochs: number of passes over entire dataset\n",
    "    batch_size: mini-batch size\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data at start of epoch\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass (through layers, then loss)\n",
    "            out = network.forward(X_batch)\n",
    "            loss = network.loss_layer.forward(out, y_batch)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # Backward pass (from loss backward through layers)\n",
    "            dX_last = network.loss_layer.backward()\n",
    "            network.backward(dX_last)\n",
    "\n",
    "            # Update weights\n",
    "            network.update_params(lr)\n",
    "\n",
    "        # Average loss over mini-batches\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6) Example usage: learn z = a*x + b*y\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # True coefficients\n",
    "    a = 2.0\n",
    "    b = 3.0\n",
    "\n",
    "    # Generate synthetic data\n",
    "    n_samples = 1000\n",
    "    # x = {1, 2, ..., 1000} / 1000\n",
    "    x = np.arange(1, n_samples + 1) / n_samples\n",
    "    y = (np.arange(1, n_samples + 1) / n_samples)[::-1]  # reverse\n",
    "    z = a * x + b * y  # true z\n",
    "    z = a * x + b * y  # true z\n",
    "\n",
    "    # Combine x, y into input X\n",
    "    X = np.column_stack((x, y))  # shape (1000, 2)\n",
    "\n",
    "    # Build a simple 2->1 network: [ConnectionLayer -> IdentityActivation]\n",
    "    # Then use an MSE loss layer\n",
    "    connection = ConnectionLayer(input_dim=2, output_dim=1)\n",
    "    activation = IdentityActivation()  # no-op, but shown for completeness\n",
    "    loss_layer = MSELossLayer()\n",
    "\n",
    "    # Create the network\n",
    "    net = SimpleNetwork(layers=[connection, activation],\n",
    "                        loss_layer=loss_layer)\n",
    "\n",
    "    # Train\n",
    "    train_network(net, X, z, epochs=30, batch_size=32, lr=0.01)\n",
    "\n",
    "    # Retrieve learned weights\n",
    "    W_learned = connection.W  # shape (2, 1)\n",
    "    C11, C12 = W_learned[0, 0], W_learned[1, 0]\n",
    "\n",
    "    print(f\"\\nLearned coefficients: C11 = {C11:.4f}, C12 = {C12:.4f}\")\n",
    "    print(f\"True coefficients: a = {a}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abaiesu/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1549 - val_loss: 0.0166\n",
      "Epoch 2/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0207 - val_loss: 0.0129\n",
      "Epoch 3/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0165 - val_loss: 0.0123\n",
      "Epoch 4/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - val_loss: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0080\n",
      "Epoch 6/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0090 - val_loss: 0.0074\n",
      "Epoch 7/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 8/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0063 - val_loss: 0.0046\n",
      "Epoch 9/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 10/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0033\n",
      "Epoch 11/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 12/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 13/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 14/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 15/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 16/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 18/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 9.3568e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3975e-04 - val_loss: 6.6711e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2867e-04 - val_loss: 5.1435e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9983e-04 - val_loss: 4.6265e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0630e-04 - val_loss: 5.5671e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5823e-04 - val_loss: 3.8227e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2183e-04 - val_loss: 3.5554e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4096e-04 - val_loss: 3.7064e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2179e-04 - val_loss: 3.7317e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8466e-04 - val_loss: 3.6289e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5857e-04 - val_loss: 2.4339e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3981e-04 - val_loss: 2.1819e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1810e-04 - val_loss: 1.8521e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9222e-04 - val_loss: 1.9397e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9067e-04 - val_loss: 1.9878e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.9311e-04 - val_loss: 1.5776e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5112e-04 - val_loss: 2.0092e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6205e-04 - val_loss: 1.7653e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5854e-04 - val_loss: 1.5282e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3871e-04 - val_loss: 1.1181e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1859e-04 - val_loss: 1.3775e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1462e-04 - val_loss: 1.4515e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3610e-04 - val_loss: 9.1855e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0990e-04 - val_loss: 1.2787e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0627e-04 - val_loss: 9.3391e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0465e-04 - val_loss: 1.0001e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0179e-04 - val_loss: 9.3879e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0388e-04 - val_loss: 9.4457e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2735e-05 - val_loss: 1.1477e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.7823e-05 - val_loss: 8.0078e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9804e-05 - val_loss: 8.3424e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5349e-05 - val_loss: 9.1668e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4322e-05 - val_loss: 6.5990e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.7496e-05 - val_loss: 7.0368e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2130e-05 - val_loss: 8.5636e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2688e-05 - val_loss: 8.8204e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4352e-05 - val_loss: 6.4295e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7304e-05 - val_loss: 6.0460e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.3031e-05 - val_loss: 7.8927e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5778e-05 - val_loss: 6.4143e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1508e-05 - val_loss: 5.5401e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8694e-05 - val_loss: 7.0721e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.3915e-05 - val_loss: 5.7247e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.1908e-05 - val_loss: 5.8675e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0942e-05 - val_loss: 5.1009e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8135e-05 - val_loss: 5.8728e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9568e-05 - val_loss: 1.2311e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2428e-05 - val_loss: 5.3251e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7609e-05 - val_loss: 5.1695e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0826e-05 - val_loss: 7.3457e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2552e-05 - val_loss: 6.8218e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7311e-05 - val_loss: 4.5171e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0221e-05 - val_loss: 4.3448e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9239e-05 - val_loss: 5.4513e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.1509e-05 - val_loss: 8.9192e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5049e-05 - val_loss: 5.1327e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6278e-05 - val_loss: 4.4774e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7936e-05 - val_loss: 4.7554e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7252e-05 - val_loss: 4.9651e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0761e-05 - val_loss: 4.8513e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2971e-05 - val_loss: 4.9466e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6328e-05 - val_loss: 3.7472e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4920e-05 - val_loss: 4.0263e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4727e-05 - val_loss: 5.4685e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0390e-05 - val_loss: 4.7647e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2949e-05 - val_loss: 3.7430e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3861e-05 - val_loss: 3.9860e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0108e-05 - val_loss: 5.3587e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5010e-05 - val_loss: 4.0688e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2014e-05 - val_loss: 4.0432e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9055e-05 - val_loss: 4.4734e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1107e-05 - val_loss: 3.9104e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4191e-05 - val_loss: 4.4030e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9898e-05 - val_loss: 3.2829e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1980e-05 - val_loss: 4.3824e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3853e-05 - val_loss: 4.4567e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7801e-05 - val_loss: 3.3632e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9658e-05 - val_loss: 3.7086e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8938e-05 - val_loss: 4.1530e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8338e-05 - val_loss: 3.4644e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9520e-05 - val_loss: 3.6002e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6010e-05 - val_loss: 3.2192e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7855e-05 - val_loss: 4.1615e-05\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0633e-05 \n",
      "Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic training data\n",
    "def generate_data(num_samples=10000):\n",
    "    x = np.random.uniform(-1, 1, (num_samples, 2))  # (x, y) pairs in range [-1,1]\n",
    "    y = np.sin(np.pi * x[:, 0]) * np.cos(np.pi * x[:, 1])  # Compute f(x,y)\n",
    "    return x, y\n",
    "\n",
    "# Generate training and test data\n",
    "X_train, y_train = generate_data(10000)\n",
    "X_test, y_test = generate_data(1000)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(2,)),  # Hidden layer 1\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer 2\n",
    "    keras.layers.Dense(64, activation='relu'),  # Hidden layer 3\n",
    "    keras.layers.Dense(1, activation='linear')  # Output layer (regression)\n",
    "])\n",
    "\n",
    "# Use SGD optimizer with momentum\n",
    "sgd_optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=sgd_optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "True: 0.2718, Predicted: -1.7613\n",
      "True: 0.6089, Predicted: -2.1386\n",
      "True: 0.2871, Predicted: -0.9146\n",
      "True: -0.2388, Predicted: -1.0114\n",
      "True: -0.1170, Predicted: -3.4494\n"
     ]
    }
   ],
   "source": [
    "# Predict and visualize results\n",
    "x_vals = np.random.uniform(0, 10, size=5)  # Generates 100 random values in [0, 10]\n",
    "y_vals = np.random.uniform(0, 10, size=5)\n",
    "true_labal = np.sin(np.pi * x_vals) * np.cos(np.pi * y_vals)\n",
    "predicted_label = model.predict(np.c_[x_vals, y_vals])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"True: {true_labal[i]:.4f}, Predicted: {predicted_label[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and visualize results\n",
    "x_vals = np.linspace(-1, 1, 100)\n",
    "y_vals = np.linspace(-1, 1, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x_vals, y_vals)\n",
    "Z_actual = np.sin(np.pi * X_grid) * np.cos(np.pi * Y_grid)  # True function\n",
    "Z_predicted = model.predict(np.c_[X_grid.ravel(), Y_grid.ravel()]).reshape(X_grid.shape)\n",
    "\n",
    "# Plot actual vs predicted function\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Actual function\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X_grid, Y_grid, Z_actual, cmap='viridis')\n",
    "ax1.set_title(\"Actual Function: sin(πx)cos(πy)\")\n",
    "\n",
    "# Predicted function\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(X_grid, Y_grid, Z_predicted, cmap='plasma')\n",
    "ax2.set_title(\"Neural Network Prediction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abaiesu/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1982\n",
      "Epoch 2/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1069\n",
      "Epoch 3/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1022\n",
      "Epoch 4/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0969\n",
      "Epoch 5/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0954\n",
      "Epoch 6/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0846\n",
      "Epoch 7/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0720\n",
      "Epoch 8/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0509\n",
      "Epoch 9/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0288\n",
      "Epoch 10/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0218\n",
      "Epoch 11/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0177\n",
      "Epoch 12/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0148\n",
      "Epoch 13/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0136\n",
      "Epoch 14/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0117\n",
      "Epoch 15/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0111\n",
      "Epoch 16/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0096\n",
      "Epoch 17/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0086\n",
      "Epoch 18/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0080\n",
      "Epoch 19/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0075\n",
      "Epoch 20/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0068\n",
      "Epoch 21/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0062\n",
      "Epoch 22/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0064\n",
      "Epoch 23/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0059\n",
      "Epoch 24/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0055\n",
      "Epoch 25/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0054\n",
      "Epoch 26/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0050\n",
      "Epoch 27/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0047\n",
      "Epoch 28/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0045\n",
      "Epoch 29/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0046\n",
      "Epoch 30/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0040\n",
      "Epoch 31/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0042\n",
      "Epoch 32/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0037\n",
      "Epoch 33/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0036\n",
      "Epoch 34/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0035\n",
      "Epoch 35/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0034\n",
      "Epoch 36/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033\n",
      "Epoch 37/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0034\n",
      "Epoch 38/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0031\n",
      "Epoch 39/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030\n",
      "Epoch 40/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028\n",
      "Epoch 41/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028\n",
      "Epoch 42/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027\n",
      "Epoch 43/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 45/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0025\n",
      "Epoch 46/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0024\n",
      "Epoch 47/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 48/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 49/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 50/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 51/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0020\n",
      "Epoch 52/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0019\n",
      "Epoch 53/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0018\n",
      "Epoch 54/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0018\n",
      "Epoch 55/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0018\n",
      "Epoch 56/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0017\n",
      "Epoch 57/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0016\n",
      "Epoch 58/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0015\n",
      "Epoch 59/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015\n",
      "Epoch 60/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0015\n",
      "Epoch 61/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0014\n",
      "Epoch 62/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014\n",
      "Epoch 63/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0013\n",
      "Epoch 64/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0014\n",
      "Epoch 65/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013\n",
      "Epoch 66/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013\n",
      "Epoch 67/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0012\n",
      "Epoch 68/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0012\n",
      "Epoch 69/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012\n",
      "Epoch 70/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011\n",
      "Epoch 71/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0011\n",
      "Epoch 72/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0011\n",
      "Epoch 73/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0010   \n",
      "Epoch 74/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.5480e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.9852e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.0540e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.4778e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.9706e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.5686e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.3004e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.4568e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2105e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8092e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4831e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6847e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8709e-04   \n",
      "Epoch 90/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.3524e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2352e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.8666e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0274e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6624e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9390e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3643e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5378e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3234e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.4482e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8831e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Input: x=0.20, y=0.50 | Predicted z=-0.0053 | True z=0.0000\n",
      "Input: x=0.70, y=0.30 | Predicted z=0.4630 | True z=0.4755\n",
      "Input: x=0.90, y=0.80 | Predicted z=-0.2291 | True z=-0.2500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the true function\n",
    "def true_function(x, y):\n",
    "    return np.sin(np.pi * x) * np.cos(np.pi * y)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.random.rand(n_samples)\n",
    "z = true_function(x, y)  # True z values\n",
    "\n",
    "# Combine x and y into input data\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "# Define the neural network model with multiple layers and tanh activations\n",
    "model = Sequential([\n",
    "    Dense(16, input_dim=2, activation='tanh', use_bias=False),  # First hidden layer\n",
    "    Dense(32, activation='tanh', use_bias=False),  # Second hidden layer\n",
    "    Dense(64, activation='relu', use_bias=False),  # Third hidden layer\n",
    "    Dense(16, activation='relu', use_bias=False),  # Fourth hidden layer\n",
    "    Dense(1, activation='linear', use_bias=False)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, z, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Test the model on a few random samples\n",
    "test_samples = np.array([[0.2, 0.5], [0.7, 0.3], [0.9, 0.8]])\n",
    "predictions = model.predict(test_samples)\n",
    "true_values = true_function(test_samples[:, 0], test_samples[:, 1])\n",
    "\n",
    "# Print results\n",
    "for i in range(len(test_samples)):\n",
    "    print(f\"Input: x={test_samples[i, 0]:.2f}, y={test_samples[i, 1]:.2f} | \"\n",
    "          f\"Predicted z={predictions[i, 0]:.4f} | True z={true_values[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
